# ğŸš€ Guide de DÃ©marrage Rapide - VLM-Bot

## Installation en 5 minutes

### 1. CrÃ©er l'environnement Conda

```bash
# CrÃ©er l'environnement 'rag' avec Python 3.10
conda create -n rag python=3.10 -y

# Activer
conda activate rag
```

### 2. Installer PyTorch + CUDA

```bash
# VÃ©rifier votre version CUDA (vous avez CUDA 13.1 compatible)
nvidia-smi

# âœ… RECOMMANDÃ‰ pour RTX 3050 (4GB VRAM) - CUDA 12.1
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y

# Note: PyTorch officiel ne supporte pas encore CUDA 13.x directement
# Mais CUDA 12.1 est rÃ©trocompatible avec votre driver 591.59
# Alternative si problÃ¨me: CUDA 11.8
# conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y

# OU CPU uniquement (pas de GPU, plus lent)
# conda install pytorch torchvision torchaudio cpuonly -c pytorch -y
```

### 3. Installer les dÃ©pendances

```bash
cd VLM-Bot
pip install -r requirements.txt
```

### 4. Configuration

```bash
# Copier le template
cp .env.example .env

# Ã‰diter .env avec votre token HuggingFace
# Obtenir un token: https://huggingface.co/settings/tokens
# Ajouter: HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxx
```

### 5. Construire l'index RAG

```bash
python scripts/build_index.py
```

Attendez 2-3 minutes pendant le tÃ©lÃ©chargement et l'indexation du dataset mÃ©dical.

### 6. Lancer l'application

```bash
python src/app.py
```

Ouvrir dans votre navigateur: http://localhost:7860

---

## VÃ©rification rapide

```bash
# VÃ©rifier l'installation
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')"

# VÃ©rifier les dÃ©pendances
python -c "import transformers, langchain, cv2, gradio; print('âœ… Toutes les dÃ©pendances sont installÃ©es')"
```

---

## Commandes utiles

```bash
# Activer l'environnement
conda activate rag

# DÃ©sactiver
conda deactivate

# Lister les packages installÃ©s
conda list

# Mettre Ã  jour une dÃ©pendance
pip install --upgrade transformers

# Reconstruire l'index RAG
python scripts/build_index.py

# Lancer l'app avec partage public (Gradio share link)
# Dans config.yaml, changer: share: true
python src/app.py
```

---

## Structure des fichiers

```
VLM-Bot/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py                    # â† Lancer ceci
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ vlm_service.py        # Phi-3-Vision
â”‚   â”‚   â”œâ”€â”€ rag_service.py        # FAISS
â”‚   â”‚   â””â”€â”€ opencv_service.py     # Extraction features
â”‚   â””â”€â”€ utils/helpers.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ build_index.py            # â† Lancer en premier
â”‚
â”œâ”€â”€ data/processed/
â”‚   â””â”€â”€ faiss_index/              # Index gÃ©nÃ©rÃ©
â”‚
â”œâ”€â”€ config.yaml                   # Configuration centrale
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env                          # Vos secrets (Ã  crÃ©er)
```

---

## Troubleshooting rapide

### âŒ ModuleNotFoundError

```bash
pip install -r requirements.txt
```

### âŒ CUDA not available

```bash
# VÃ©rifier CUDA
nvidia-smi

# RÃ©installer PyTorch avec CUDA
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y
```

### âŒ HuggingFace authentication error

```bash
# Login manuel
huggingface-cli login
# Entrer votre token
```

### âŒ Out of Memory (OOM) GPU

Dans `config.yaml`:
```yaml
models:
  vlm:
    max_memory:
      0: "3.5GB"  # RÃ©duire si nÃ©cessaire
```

### âŒ Index RAG non trouvÃ©

```bash
python scripts/build_index.py
```

---

## Support

- ğŸ“– README complet: [README.md](README.md)
- ğŸ› Issues: [GitHub Issues](https://github.com/votre-repo/issues)
- ğŸ“§ Email: support@vlm-bot.example

---

**Version Python recommandÃ©e**: 3.10  
**TestÃ© sur**: Windows 11, Ubuntu 22.04, macOS (CPU)
